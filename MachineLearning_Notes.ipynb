{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports - Basics\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reading Data from Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read CSV\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/upxacademy/ML_with_Python/master/Datasets/glass.data?token=AH0Y7F-9aM-g_uOfpoUhYRF3xm7oJjrzks5ZFEl1wA%3D%3D'\n",
    "col_names = ['index_col_name','feature_1','feature_2','feature_3','feature_4','label_col_name']\n",
    "dataframe = pd.read_csv(url, names=col_names, index_col='index_col_name')\n",
    "dataframe.sort_values(by='feature_2', inplace=True)\n",
    "#dataframe.head()\n",
    "\n",
    "# Outter Join 2 sets\n",
    "dataframe_1 = pd.read_csv(\"student-mat.csv\",delimiter=\";\")\n",
    "dataframe_2 = pd.read_csv(\"student-por.csv\",delimiter=\";\")\n",
    "dataframe_combined = pd.merge(dataframe_1,dataframe_2,how=\"outer\")\n",
    "\n",
    "\n",
    "# OR read the csv from URL and save as local file\n",
    "import urllib.request\n",
    "\n",
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand-training-true.data\"\n",
    "urllib.request.urlretrieve(url, \"poker_train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read ARFF\n",
    "\n",
    "from scipy.io.arff import loadarff\n",
    "\n",
    "data_raw = loadarff(\"PhishingData.arff\")\n",
    "# Selection of the Data and converting data into numpy format for flexibility in cleaning\n",
    "numpy_data_array = np.array(data_raw[0])\n",
    "# Converting the numpy array into Pandas data frame and casting the coloumns to numeric type\n",
    "dataframe = pd.DataFrame(numpy_data_array).apply(pd.to_numeric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read from URL\n",
    "\n",
    "import requests\n",
    "\n",
    "http_request = requests.get('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv')\n",
    "http_text = http_request.text.split(\"\\n\")\n",
    "data_array = []\n",
    "for lines in http_text:\n",
    "    data_array.append(lines.split(\";\"))\n",
    "col_names = []\n",
    "for col in data_array[0]:\n",
    "    col_names.append(col.strip('\"'))\n",
    "\n",
    "dataframe = pd.DataFrame(data=data_array[1:],columns=col_names).apply(pd.to_numeric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Massage of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Handle Categorial Feature - Create dummy columns\n",
    "\n",
    "col_str = original_dataframe.columns[original_dataframe.dtypes == object]\n",
    "final_dataframe = pd.get_dummies(original_dataframe, columns = col_str, drop_first = True) \n",
    "\n",
    "# step by step\n",
    "# ============\n",
    "#dummies = pd.get_dummies(original_dataframe.target_col_name, prefix='target_col_name_prefix')\n",
    "#dummies.drop(dummies.columns[0], axis=1, inplace=True)\n",
    "#original_dataframe = pd.concat([original_dataframe, dummies], axis=1)\n",
    "\n",
    "# Simple Way\n",
    "# ==========\n",
    "#dataframe[\"Sex\"] = dataframe[\"Sex\"].apply(lambda sex: 0 if sex == 'male' else 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Handle NaN values - Imputer\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "feature_cols = ['feature_1','feature_2','feature_3','feature_4']\n",
    "features_original = dataframe[list(feature_cols)].values\n",
    "\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "features_transformed = imp.fit_transform(features_original)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Analysing Data by Plotting Graphs\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#plt.rcParams['figure.figsize'] = (8, 6)\n",
    "#plt.rcParams['font.size'] = 14\n",
    "\n",
    "# Pandas scatter plot\n",
    "dataframe_name.plot(kind='scatter', x='feature_name', y='label_name', alpha=0.2)\n",
    "# multiple scatter plots in Pandas\n",
    "feature_cols = ['feature_1', 'feature_2', 'feature_3', 'feature_4']\n",
    "fig, axs = plt.subplots(1, len(feature_cols), sharey=True)\n",
    "for index, feature in enumerate(feature_cols):\n",
    "    bikes.plot(kind='scatter', x=feature, y='label_name', ax=axs[index], figsize=(16, 3))\n",
    "\n",
    "    \n",
    "import seaborn as sns\n",
    "\n",
    "# Seaborn scatter plot with regression line\n",
    "sns.lmplot(x='feature_name', y='label_name', data=dataframe_name, aspect=1.5, scatter_kws={'alpha':0.2})\n",
    "# multiple scattered plot using Seaborn\n",
    "feature_cols = ['feature_1', 'feature_2', 'feature_3', 'feature_4']\n",
    "sns.pairplot(dataframe_name, x_vars=feature_cols, y_vars='label_name', kind='reg')\n",
    "\n",
    "# Box Plot for Logistic\n",
    "sns.boxplot(x='label_name', y='feature_name', data=dataframe_name)\n",
    "# Scatter Plot for Logistic\n",
    "sns.lmplot(x='feature_1', y='feature_2', hue = 'label_name', data=dataframe_name, aspect=1.5, ci = None, fit_reg = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removing Low Variance Features - PCA\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "label = dataframe[\"label_name\"].values\n",
    "predictors = dataframe.drop(axis = 1,labels= [\"label_name\"]).values\n",
    "\n",
    "pca = PCA(n_components=len(features.columns)-1)\n",
    "pca.fit(predictors)\n",
    "\n",
    "#explained variance ratios are sorted descending (highest variance feature at the begining)\n",
    "variance_ratio_cum_sum=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
    "print(variance_ratio_cum_sum) #then able to find top variance feature\n",
    "plt.plot(variance_ratio_cum_sum)\n",
    "\n",
    "#Looking at above plot let's say taking (first) 10 variables (= top 10 highest variance features)\n",
    "pca = PCA(n_components=10)\n",
    "pca.fit(predictors)\n",
    "transformed_features = pca.fit_transform(predictors) #can then be used for whatever model, say LinearRegression\n",
    "\n",
    "\n",
    "# Cross Validation Score to double-check difference between transformed VS original\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "linear_txf = linear_model.LinearRegression()\n",
    "score_lr_txf = cross_val_score(linear_txf, transformed_features, label, cv=5) # cv=5 -> 5-fold validation\n",
    "print(\"LR Model Cross Validation score : \" + str(score_lr_txf))\n",
    "print(\"LR Model Cross Validation Mean score : \" + str(score_lr_txf.mean()))\n",
    "lr = linear_model.LinearRegression()\n",
    "score_lr = cross_val_score(lr, predictors, label, cv=5)\n",
    "print(\"LR Model Cross Validation score : \" + str(score_lr))\n",
    "print(\"LR Model Cross Validation Mean score : \" + str(score_lr.mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize data (variables of different features to same scale)\n",
    "#\n",
    "# Feature(s) with large range , with different features of different scale/range/metric\n",
    "# Important features got loss / biased\n",
    "#\n",
    "# Except DecisionTree and RandomForest, can improve performance / result in certain cases\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_model = MinMaxScaler().fit_transform(features.values)\n",
    "bins = np.linspace(0, 1, 10)\n",
    "digitized_features = np.digitize(scaler_model, bins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preparation of Data for Training/Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting Data into Training and Testing sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "feature_columns = list(['feature_1','feature_2','feature_3','feature_4'])\n",
    "label_columns = \"label_name\"\n",
    "features = dataframe[feature_columns].values\n",
    "labels = dataframe[label_columns].values\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, random_state=42)\n",
    "#features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.3, random_state=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(features_train, labels_train)\n",
    "\n",
    "print(linreg.intercept_)\n",
    "list(zip(feature_cols, linreg.coef_))\n",
    "\n",
    "labels_predicted = linreg.predict(features_test)\n",
    "\n",
    "print(np.sqrt(metrics.mean_squared_error(labels_test, labels_predicted)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(C=1e42) # Set Large C value for low regularization\n",
    "logreg.fit(features_train, labels_train)\n",
    "\n",
    "print(logreg.intercept_)\n",
    "list(zip(feature_cols, logreg.coef_))\n",
    "\n",
    "labels_predicted = logreg.predict(features_test) # Categorial / Digital\n",
    "predicted_proba = logreg.predict_proba(features_test) # Analog (e.g. > 0.5 = true else false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=6)\n",
    "#decision_tree = DecisionTreeClassifier(criterion=\"gini\", max_depth=5)\n",
    "decision_tree = decision_tree.fit(features_train, labels_train)\n",
    "\n",
    "print(decision_tree.feature_importances_)\n",
    "    \n",
    "print(decision_tree.score(features_train, labels_train))\n",
    "\n",
    "predicted_proba = decision_tree.predict_proba(features_test)\n",
    "# Get success rate of the model\n",
    "print(roc_auc_score(labels_test, predicted_proba[:,1])) # [:,1] for only 2 classes (e.g. Churn, Not Churn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "random_forest = RandomForestClassifier(criterion=\"entropy\", max_depth = 10, min_samples_split=2, n_estimators = 100, random_state = 1)\n",
    "#random_forest = RandomForestClassifier(criterion=\"gini\", max_depth = 10, min_samples_split=2, n_estimators = 100, random_state = 1)\n",
    "random_forest = random_forest.fit(features_train, labels_train)\n",
    "\n",
    "print(random_forest.feature_importances_)\n",
    "\n",
    "print(random_forest.score(features_train, labels_train))\n",
    "\n",
    "predicted_proba = random_forest.predict_proba(features_test)\n",
    "# Get success rate of the model\n",
    "print(roc_auc_score(labels_test, predicted_proba[:,1])) # [:,1] for only 2 classes (e.g. Churn, Not Churn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# kNN : k - Near Neighbors (Classification)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=10, weights='distance')\n",
    "knn = knn.fit(features_train, labels_train)\n",
    "\n",
    "print(knn.score(features_train, labels_train))\n",
    "\n",
    "predicted_proba = knn.predict_proba(features_test)\n",
    "# Get success rate of the model\n",
    "print(roc_auc_score(labels_test, predicted_proba[:,1])) # [:,1] for only 2 classes (e.g. Legitimate or Malicious site)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Naive Bayes (Classification, strong independence between features)\n",
    "\n",
    "# Feature Extraction - http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#TFIDF Vectorizer\n",
    "stopset = set(stopwords.words('english'))\n",
    "vectorizer = TfidfVectorizer(use_idf=True, lowercase=True, strip_accents='ascii', stop_words=stopset)\n",
    "features = vectorizer.fit_transform(dataframe.text) # transform sentence text from 1 column to features\n",
    "\n",
    "\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#clf = naive_bayes.BernoulliNB() #discrete data, for binary/boolean features\n",
    "#clf = naive_bayes.GaussianNB() # ?\n",
    "clf = naive_bayes.MultinomialNB() #discrete data, for occurrence count\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "predicted_proba = clf.predict_proba(features_test)\n",
    "# Get success rate of the model\n",
    "print(roc_auc_score(labels_test, predicted_proba[:,1])) # [:,1] for only 2 classes (e.g. Spam or Ham)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SVM : Support Vector Machine\n",
    "# Maximize margin around the separating hyperplane\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = svm.SVC() #C-Support Vector Classification\n",
    "clf.fit(X=digitized,y=label.values)  \n",
    "\n",
    "score_knn = cross_val_score(clf, digitized, label.values, cv=4)\n",
    "print(\"Cross Validation score : \" + str(score_knn))\n",
    "print(\"Cross Validation Mean score : \" + str(score_knn.mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Presentation of Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "def plot_confusion_matrix(df_confusion, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(df_confusion, cmap=cmap)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    plt.yticks(np.arange(2), ('False', 'True'))\n",
    "    plt.xticks(np.arange(2), ('False', 'True'))\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    \n",
    "    thresh = df_confusion.max() / 2\n",
    "    for i, j in itertools.product(range(df_confusion.shape[0]), range(df_confusion.shape[1])):\n",
    "        plt.text(j, i, df_confusion[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if df_confusion[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "labels_pred = whatever_model.predict(features)\n",
    "df_confusion = metrics.confusion_matrix(labels, labels_pred)\n",
    "#df_confusion\n",
    "plot_confusion_matrix(df_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unsupervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clustering with k-Means\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Elbow Method - to select the optimum k\n",
    "#   https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set\n",
    "#\n",
    "# Increasing k without penalty \n",
    "#    will always reduce the amount of error in the resulting clustering, \n",
    "#    to the extreme case of zero error if each data point is considered its own cluster \n",
    "#      (i.e., when k equals the number of data points, n)\n",
    "# the optimal choice of k will strike a balance between \n",
    "#   - maximum compression of the data using a single cluster, and \n",
    "#   - maximum accuracy by assigning each data point to its own cluster\n",
    "#\n",
    "# When K increases, the centroids are closer to the clusters centroids\n",
    "# The improvements will decline, at some point rapidly, creating the elbow shape\n",
    "\n",
    "num_of_clusters=range(1,11) # k-means cluster analysis for 1-10 clusters\n",
    "mean_dist=[]\n",
    "\n",
    "for k in num_of_clusters:\n",
    "    model=KMeans(n_clusters=k)\n",
    "    model.fit(predictors)\n",
    "    mean_dist.append( sum(np.min(cdist(predictors, model.cluster_centers_, 'euclidean'), axis=1)) / predictors.shape[0] )\n",
    "\n",
    "plt.plot(num_of_clusters, mean_dist)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Average distance')\n",
    "plt.title('Selecting k with the Elbow Method') # pick the fewest number of clusters that reduces the average distance\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
